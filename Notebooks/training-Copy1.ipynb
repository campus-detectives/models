{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0694dec3-e4c1-4a6c-8684-ce5a771008c7",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14c3505-d08b-452e-a29a-f70e5ec973c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "#Importing image file paths\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "#For managing dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#PyTorch\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "#Metrics\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Selecting device\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b9f07-7b6c-4e03-a2ca-153fab532c7c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18692f1-4e26-4ac6-9a17-6d7cc5ce0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, path, shuffle_pairs=True, augment=False):\n",
    "        '''\n",
    "        Create an iterable dataset from a directory containing sub-directories of \n",
    "        entities with their images contained inside each sub-directory.\n",
    "\n",
    "            Parameters:\n",
    "                    path (str):                 Path to directory containing the dataset.\n",
    "                    shuffle_pairs (boolean):    Pass True when training, False otherwise. When set to false, the image pair generation will be deterministic\n",
    "                    augment (boolean):          When True, images will be augmented using a standard set of transformations.\n",
    "\n",
    "            where b = batch size\n",
    "\n",
    "            Returns:\n",
    "                    output (torch.Tensor): shape=[b, 1], Similarity of each pair of images\n",
    "        '''\n",
    "        self.path = path\n",
    "\n",
    "        self.feed_shape = [3, 224, 224]\n",
    "        self.shuffle_pairs = shuffle_pairs\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "        if self.augment:\n",
    "            # If images are to be augmented, add extra operations for it (first two).\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=0.2),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.Resize(self.feed_shape[1:])\n",
    "            ])\n",
    "        else:\n",
    "            # If no augmentation is needed then apply only the normalization and resizing operations.\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.Resize(self.feed_shape[1:])\n",
    "            ])\n",
    "\n",
    "        self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        '''\n",
    "        Creates two lists of indices that will form the pairs, to be fed for training or evaluation.\n",
    "        '''\n",
    "\n",
    "        self.image_paths = glob.glob(os.path.join(self.path, \"*/*.jpg\"))\n",
    "        self.image_classes = []\n",
    "        self.class_indices = {}\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            image_class = image_path.split(os.path.sep)[-2]\n",
    "            self.image_classes.append(image_class)\n",
    "\n",
    "            if image_class not in self.class_indices:\n",
    "                self.class_indices[image_class] = []\n",
    "            self.class_indices[image_class].append(self.image_paths.index(image_path))\n",
    "\n",
    "        self.indices1 = np.arange(len(self.image_paths))\n",
    "\n",
    "        if self.shuffle_pairs:\n",
    "            np.random.seed(int(time.time()))\n",
    "            np.random.shuffle(self.indices1)\n",
    "        else:\n",
    "            # If shuffling is set to off, set the random seed to 1, to make it deterministic.\n",
    "            np.random.seed(1)\n",
    "\n",
    "        select_pos_pair = np.random.rand(len(self.image_paths)) < 0.5\n",
    "\n",
    "        self.indices2 = []\n",
    "\n",
    "        for i, pos in zip(self.indices1, select_pos_pair):\n",
    "            class1 = self.image_classes[i]\n",
    "            if pos:\n",
    "                class2 = class1\n",
    "            else:\n",
    "                class2 = np.random.choice(list(set(self.class_indices.keys()) - {class1}))\n",
    "            idx2 = np.random.choice(self.class_indices[class2])\n",
    "            self.indices2.append(idx2)\n",
    "        self.indices2 = np.array(self.indices2)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.create_pairs()\n",
    "\n",
    "        for idx, idx2 in zip(self.indices1, self.indices2):\n",
    "\n",
    "            image_path1 = self.image_paths[idx]\n",
    "            image_path2 = self.image_paths[idx2]\n",
    "\n",
    "            class1 = self.image_classes[idx]\n",
    "            class2 = self.image_classes[idx2]\n",
    "\n",
    "            image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "            image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "\n",
    "            if self.transform:\n",
    "                image1 = self.transform(image1).float()\n",
    "                image2 = self.transform(image2).float()\n",
    "\n",
    "            yield (image1, image2), torch.FloatTensor([class1==class2]), (class1, class2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b770ed5-3cf7-421f-8345-b5ce90a3a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=[]\n",
    "image_fetch(\"../DATASET/\",image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e603a4f-6336-4b73-ad8d-e13fff2b4c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d1=defaultdict(int)\n",
    "for i in range(len(image_path)):\n",
    "    d1[image_path[i].split(\"/\")[-2]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8353300b-9d7c-4ee1-b061-f6b7e1df23a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'NOTEBOOK': 15,\n",
       "             'LUNCH BOX': 4,\n",
       "             'PEN1': 7,\n",
       "             'PHONE': 11,\n",
       "             'HEADPHONES': 3,\n",
       "             'METAL BOTTLE': 8,\n",
       "             'PLASTIC BOTTLE': 9,\n",
       "             'WALLET': 15,\n",
       "             'PEN4': 4,\n",
       "             'MOBILE CHARGER': 5,\n",
       "             'PENCIL KIT 2': 16,\n",
       "             'HEADPHONES 2': 3,\n",
       "             'PEN3': 5,\n",
       "             'KEYS 2': 3,\n",
       "             'LAPTOP CHARGER': 3,\n",
       "             'CALCULATOR 2': 4,\n",
       "             'AIRPODS': 7,\n",
       "             'KEYS': 8,\n",
       "             'TEXTBOOK': 10,\n",
       "             'WALLET 2': 5,\n",
       "             'PEN2': 7,\n",
       "             'PENCIL': 6,\n",
       "             'SCALE': 9,\n",
       "             'BAG': 4,\n",
       "             'CALCULATOR': 5,\n",
       "             'PENCIL KIT': 3,\n",
       "             'ID CARD FULL': 4,\n",
       "             'EARPHONES': 5})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b6b00-1182-45ee-abb5-b1893d2bbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaimeseNetworkDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "\n",
    "        self.image_path=[]\n",
    "        image_fetch(folder_path,self.image_path)\n",
    "\n",
    "        self.class_count=defaultdict(int)\n",
    "    \n",
    "        for i in range(len(self.image_path)):\n",
    "            self.class_count[self.image_path[i].split(\"/\")[-2]]+=1\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def image_fetch(src,image_paths,file=None):\n",
    "        l=os.listdir(src)\n",
    "        if(len(l)!=0):\n",
    "            for i in range(len(l)):\n",
    "                if(file!=None):\n",
    "                    if(file in l[i]):\n",
    "                        image_paths.append(str(src+l[i]))\n",
    "                elif(\".\" not in l[i]):\n",
    "                    try:\n",
    "                        image_fetch(str(src+\"/\"+l[i]+\"/\"),image_paths)\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    if(\".jpeg\" in l[i] or \".png\" in l[i] or \".jpg\" in l[i]):\n",
    "                        image_paths.append(str(src+l[i]))\n",
    "                            \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f5f74-04ed-445d-9913-fbb7116a9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "271b233e-2664-4267-8806-63605ed035f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "NOTEBOOK\n",
      "15\n",
      "LUNCH BOX\n",
      "4\n",
      "LUNCH BOX\n",
      "4\n",
      "LUNCH BOX\n",
      "4\n",
      "LUNCH BOX\n",
      "4\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PEN1\n",
      "7\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "PHONE\n",
      "11\n",
      "HEADPHONES\n",
      "3\n",
      "HEADPHONES\n",
      "3\n",
      "HEADPHONES\n",
      "3\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "METAL BOTTLE\n",
      "8\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "PLASTIC BOTTLE\n",
      "9\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "WALLET\n",
      "15\n",
      "PEN4\n",
      "4\n",
      "PEN4\n",
      "4\n",
      "PEN4\n",
      "4\n",
      "PEN4\n",
      "4\n",
      "MOBILE CHARGER\n",
      "5\n",
      "MOBILE CHARGER\n",
      "5\n",
      "MOBILE CHARGER\n",
      "5\n",
      "MOBILE CHARGER\n",
      "5\n",
      "MOBILE CHARGER\n",
      "5\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "PENCIL KIT 2\n",
      "16\n",
      "HEADPHONES 2\n",
      "3\n",
      "HEADPHONES 2\n",
      "3\n",
      "HEADPHONES 2\n",
      "3\n",
      "PEN3\n",
      "5\n",
      "PEN3\n",
      "5\n",
      "PEN3\n",
      "5\n",
      "PEN3\n",
      "5\n",
      "PEN3\n",
      "5\n",
      "KEYS 2\n",
      "3\n",
      "KEYS 2\n",
      "3\n",
      "KEYS 2\n",
      "3\n",
      "LAPTOP CHARGER\n",
      "3\n",
      "LAPTOP CHARGER\n",
      "3\n",
      "LAPTOP CHARGER\n",
      "3\n",
      "CALCULATOR 2\n",
      "4\n",
      "CALCULATOR 2\n",
      "4\n",
      "CALCULATOR 2\n",
      "4\n",
      "CALCULATOR 2\n",
      "4\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "AIRPODS\n",
      "7\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "KEYS\n",
      "8\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "TEXTBOOK\n",
      "10\n",
      "WALLET 2\n",
      "5\n",
      "WALLET 2\n",
      "5\n",
      "WALLET 2\n",
      "5\n",
      "WALLET 2\n",
      "5\n",
      "WALLET 2\n",
      "5\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PEN2\n",
      "7\n",
      "PENCIL\n",
      "6\n",
      "PENCIL\n",
      "6\n",
      "PENCIL\n",
      "6\n",
      "PENCIL\n",
      "6\n",
      "PENCIL\n",
      "6\n",
      "PENCIL\n",
      "6\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "SCALE\n",
      "9\n",
      "BAG\n",
      "4\n",
      "BAG\n",
      "4\n",
      "BAG\n",
      "4\n",
      "BAG\n",
      "4\n",
      "CALCULATOR\n",
      "5\n",
      "CALCULATOR\n",
      "5\n",
      "CALCULATOR\n",
      "5\n",
      "CALCULATOR\n",
      "5\n",
      "CALCULATOR\n",
      "5\n",
      "PENCIL KIT\n",
      "3\n",
      "PENCIL KIT\n",
      "3\n",
      "PENCIL KIT\n",
      "3\n",
      "ID CARD FULL\n",
      "4\n",
      "ID CARD FULL\n",
      "4\n",
      "ID CARD FULL\n",
      "4\n",
      "ID CARD FULL\n",
      "4\n",
      "EARPHONES\n",
      "5\n",
      "EARPHONES\n",
      "5\n",
      "EARPHONES\n",
      "5\n",
      "EARPHONES\n",
      "5\n",
      "EARPHONES\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for path in image_path:\n",
    "    #class_obj=path.split(path.split(\"/\")[-1])[0]\n",
    "    class_obj=path.split(\"/\")[-2]\n",
    "    \n",
    "    count=d1[class_obj]\n",
    "\n",
    "    i=count\n",
    "    while(count>0):\n",
    "        \n",
    "        i-=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24fb73b5-5797-4385-925f-8529e247b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'NOTEBOOK': 15,\n",
       "             'LUNCH BOX': 4,\n",
       "             'PEN1': 7,\n",
       "             'PHONE': 11,\n",
       "             'HEADPHONES': 3,\n",
       "             'METAL BOTTLE': 8,\n",
       "             'PLASTIC BOTTLE': 9,\n",
       "             'WALLET': 15,\n",
       "             'PEN4': 4,\n",
       "             'MOBILE CHARGER': 5,\n",
       "             'PENCIL KIT 2': 16,\n",
       "             'HEADPHONES 2': 3,\n",
       "             'PEN3': 5,\n",
       "             'KEYS 2': 3,\n",
       "             'LAPTOP CHARGER': 3,\n",
       "             'CALCULATOR 2': 4,\n",
       "             'AIRPODS': 7,\n",
       "             'KEYS': 8,\n",
       "             'TEXTBOOK': 10,\n",
       "             'WALLET 2': 5,\n",
       "             'PEN2': 7,\n",
       "             'PENCIL': 6,\n",
       "             'SCALE': 9,\n",
       "             'BAG': 4,\n",
       "             'CALCULATOR': 5,\n",
       "             'PENCIL KIT': 3,\n",
       "             'ID CARD FULL': 4,\n",
       "             'EARPHONES': 5})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "504341fd-e3d0-4145-91f6-878a5045d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import time\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, path, shuffle_pairs=True, augment=False):\n",
    "        '''\n",
    "        Create an iterable dataset from a directory containing sub-directories of \n",
    "        entities with their images contained inside each sub-directory.\n",
    "\n",
    "            Parameters:\n",
    "                    path (str):                 Path to directory containing the dataset.\n",
    "                    shuffle_pairs (boolean):    Pass True when training, False otherwise. When set to false, the image pair generation will be deterministic\n",
    "                    augment (boolean):          When True, images will be augmented using a standard set of transformations.\n",
    "\n",
    "            where b = batch size\n",
    "\n",
    "            Returns:\n",
    "                    output (torch.Tensor): shape=[b, 1], Similarity of each pair of images\n",
    "        '''\n",
    "        self.path = path\n",
    "\n",
    "        self.feed_shape = [3, 224, 224]\n",
    "        self.shuffle_pairs = shuffle_pairs\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "        if self.augment:\n",
    "            # If images are to be augmented, add extra operations for it (first two).\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=0.2),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.Resize(self.feed_shape[1:])\n",
    "            ])\n",
    "        else:\n",
    "            # If no augmentation is needed then apply only the normalization and resizing operations.\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.Resize(self.feed_shape[1:])\n",
    "            ])\n",
    "\n",
    "        self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        '''\n",
    "        Creates two lists of indices that will form the pairs, to be fed for training or evaluation.\n",
    "        '''\n",
    "\n",
    "        self.image_paths = glob.glob(os.path.join(self.path, \"*/*.jpg\"))\n",
    "        self.image_classes = []\n",
    "        self.class_indices = {}\n",
    "\n",
    "        for image_path in self.image_paths:\n",
    "            image_class = image_path.split(os.path.sep)[-2]\n",
    "            self.image_classes.append(image_class)\n",
    "\n",
    "            if image_class not in self.class_indices:\n",
    "                self.class_indices[image_class] = []\n",
    "            self.class_indices[image_class].append(self.image_paths.index(image_path))\n",
    "\n",
    "        self.indices1 = np.arange(len(self.image_paths))\n",
    "\n",
    "        if self.shuffle_pairs:\n",
    "            np.random.seed(int(time.time()))\n",
    "            np.random.shuffle(self.indices1)\n",
    "        else:\n",
    "            # If shuffling is set to off, set the random seed to 1, to make it deterministic.\n",
    "            np.random.seed(1)\n",
    "\n",
    "        select_pos_pair = np.random.rand(len(self.image_paths)) < 0.5\n",
    "\n",
    "        self.indices2 = []\n",
    "\n",
    "        for i, pos in zip(self.indices1, select_pos_pair):\n",
    "            class1 = self.image_classes[i]\n",
    "            if pos:\n",
    "                class2 = class1\n",
    "            else:\n",
    "                class2 = np.random.choice(list(set(self.class_indices.keys()) - {class1}))\n",
    "            idx2 = np.random.choice(self.class_indices[class2])\n",
    "            self.indices2.append(idx2)\n",
    "        self.indices2 = np.array(self.indices2)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.create_pairs()\n",
    "\n",
    "        for idx, idx2 in zip(self.indices1, self.indices2):\n",
    "\n",
    "            image_path1 = self.image_paths[idx]\n",
    "            image_path2 = self.image_paths[idx2]\n",
    "\n",
    "            class1 = self.image_classes[idx]\n",
    "            class2 = self.image_classes[idx2]\n",
    "\n",
    "            image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "            image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "\n",
    "            if self.transform:\n",
    "                image1 = self.transform(image1).float()\n",
    "                image2 = self.transform(image2).float()\n",
    "\n",
    "            yield (image1, image2), torch.FloatTensor([class1==class2]), (class1, class2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d277e32b-2a13-484c-9655-786b80ae4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Dataset('../DATASET/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca8763d8-d51a-4cd2-8c00-1e1604a6c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e0a96c8-2045-4230-99f7-792b7d044943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ID CARD FULL', 'AIRPODS', 'MOBILE CHARGER', 'NOTEBOOK', 'MOBILE CHARGER', 'PLASTIC BOTTLE', 'WALLET 2', 'NOTEBOOK') ('ID CARD FULL', 'PENCIL KIT', 'NOTEBOOK', 'PEN4', 'PEN3', 'PLASTIC BOTTLE', 'PEN2', 'KEYS') tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass2\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 99\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m class1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_classes[idx]\n\u001b[1;32m     97\u001b[0m class2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_classes[idx2]\n\u001b[0;32m---> 99\u001b[0m image1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m image2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path2)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/PIL/Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    865\u001b[0m ):\n\u001b[1;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for (img1, img2), y, (class1, class2) in train_dataloader:\n",
    "    print(class1,class2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c2394-5ad4-453f-b752-504a9d96a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
